Iâ€™m an AI/ML engineer and founder focused on building the next layer of intelligence infrastructure â€” systems where speed, safety, and cognition meet.

My work bridges Rust-native performance with large-scale model architecture. I design inference pipelines, latency-adaptive schedulers, and LLM optimization frameworks that push models closer to real-time intelligence.

Before Rust, I worked primarily in Python â€” building ML pipelines for finance, signal modeling, and context-aware AGI prototypes. That foundation in data and systems design evolved into Tensorust, where I now focus on production-grade Rust tools for high-performance inference.

Featured Projects

Every Other Token â€“ Rust-based LLM optimizer reducing inference latency by 48%.

LLM Affector â€“ runtime controller for token generation and contextual flow.

Adaptive Task Scheduler â€“ distributed orchestration layer for multi-model pipelines.

Mycelium-AI Platform â€“ biological sensing + AI integration for environmental feedback loops.

Core Stack
Rust, Python, PyTorch, Hugging Face Transformers, Flask, FastAPI, Docker, Linux, PostgreSQL

Focus Areas
LLM infrastructure, inference optimization, cognitive systems, Rust AI tooling

I like systems that scale under pressure â€” and the people who help build them.
If youâ€™re experimenting at the edge of AI infrastructure, Iâ€™d love to connect.

ğŸ“§ mattbusel@gmail.com
 | ğŸŒ linkedin.com/in/matthewbusel
 | ğŸ¦€ github.com/Mattbusel


