I’m an AI/ML engineer and founder focused on building the next layer of intelligence infrastructure, systems where speed, safety, and cognition meet.

My work bridges Rust-native performance with large-scale model architecture. I design inference pipelines, latency-adaptive schedulers, and LLM optimization frameworks that push models closer to real-time intelligence.

Before Rust, I worked primarily in Python, building ML pipelines for finance, and signal modeling. That foundation in data and systems design evolved into Tensorust, where I now focus on production-grade Rust tools for high-performance inference.

Featured Projects

Every Other Token – Rust-based LLM optimizer reducing inference latency by 48%.

LLM Affector – runtime controller for token generation and contextual flow.

Adaptive Task Scheduler - distributed orchestration layer for multi-model pipelines.

Mycelium-AI Platform – biological sensing + AI integration for environmental feedback loops.

Core Stack
Rust, Python, PyTorch, Hugging Face Transformers, Flask, FastAPI, Docker, Linux, PostgreSQL

Focus Areas
LLM infrastructure, inference optimization, cognitive systems, Rust AI tooling

I like systems that scale under pressure, and the people who help build them.
If you’re experimenting at the edge of AI infrastructure, I’d love to connect.

📧 mattbusel@gmail.com
 | 🌐 linkedin.com/in/matthewbusel
 | 🦀 github.com/Mattbusel


